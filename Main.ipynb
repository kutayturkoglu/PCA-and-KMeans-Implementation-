{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of the PCA and KMeans\n",
    "Here below there are the basic implementations of KMeans and PCA Analysis. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class PCA(object):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def _normalize(self, data):\n",
    "        \n",
    "        mean = np.mean(self.data, axis=0)\n",
    "        return (mean - self.data) \n",
    "    \n",
    "    def _explained_variance(self, eigen_val, eigen_vec):\n",
    "        \n",
    "        sum_eigen_val = np.sum(eigen_val)\n",
    "        explained_variance = eigen_val/sum_eigen_val\n",
    "        cumulative_variance = np.cumsum(explained_variance)\n",
    "        return explained_variance, cumulative_variance    \n",
    "        \n",
    "    def _fit(self):\n",
    "        \n",
    "        standardized_data = self._normalize(self.data)\n",
    "        covariance_matrix = np.cov(standardized_data.T).round(2)\n",
    "        eigen_val, eigen_vec = np.linalg.eig(covariance_matrix)\n",
    "\n",
    "        indices = ([x for _,x in sorted(zip(eigen_val, np.arange(0,len(eigen_val))))])[::-1]\n",
    "        eigen_val = eigen_val[indices]\n",
    "        eigen_vec = eigen_vec[:,indices]\n",
    "        \n",
    "        explained_variance, cumulative_variance = self._explained_variance(eigen_val=eigen_val,eigen_vec=eigen_vec)\n",
    "        \n",
    "        eigenvector_subset = eigen_vec[:,0:2]\n",
    "        reduced_data = np.dot(eigenvector_subset.T,standardized_data.T).T\n",
    "        \n",
    "        return reduced_data, explained_variance, cumulative_variance\n",
    "      \n",
    "class KMeans(object):\n",
    "    def __init__(self, n_clusters, centroids=None, seed=1):\n",
    "        np.random.seed(seed)\n",
    "        self.n_clusters=n_clusters\n",
    "        self.centroids=centroids\n",
    "        self.random_gen=np.random.choice\n",
    "        self.cls_labels = None\n",
    "    \n",
    "    def _euclidean (self, x, y):\n",
    "        return np.sqrt((x[0]-y[0])**2+(x[1]-y[1])**2)\n",
    "    \n",
    "    def _pick_random_centroids(self, X):\n",
    "        \n",
    "        clusters = np.zeros((self.n_clusters, 2))\n",
    "        for i in range(self.n_clusters):\n",
    "          clusters[i] = (X[self.random_gen(len(X))])\n",
    "    \n",
    "        return clusters\n",
    "    \n",
    "    def _plot_clustering(self, X, it, cls_labels):\n",
    "  \n",
    "        plt.scatter(X[:,0],X[:,1], c=cls_labels)\n",
    "        plt.title(\"Iteration: \"+str(it))\n",
    "        plt.show()\n",
    "    \n",
    "    def fit(self, X, max_iteration=100, visualize=False):\n",
    "      \n",
    "      df = pd.DataFrame(index=range(len(X)),columns=range(self.n_clusters))\n",
    "      self.centroids = self._pick_random_centroids(X)      \n",
    "      new_centroids = np.zeros((self.n_clusters,2))\n",
    "      self.cls_labels = np.zeros(len(X))\n",
    "      it = 1\n",
    "    \n",
    "      while(np.array_equal(new_centroids,self.centroids) == False):\n",
    "        clusters = [[] for i in range(len(self.centroids))]\n",
    "\n",
    "        for i in range(len(X)):\n",
    "          for j in range(len(self.centroids)):\n",
    "            df[j][i] = KMeans._euclidean(KMeans(self.n_clusters), X[i], self.centroids[j])\n",
    "        \n",
    "        for i in range(df.T.shape[1]):\n",
    "          filter = df.T[i]==min(df.T[i])\n",
    "          for j in range(len(filter)):\n",
    "            if(filter[j] == True):\n",
    "              self.cls_labels[i] = j\n",
    "              clusters[j].append(X[i])\n",
    "        \n",
    "        for i in range(len(clusters)):\n",
    "          new_centroids[i] = np.mean(clusters[i], axis = 0)\n",
    "        \n",
    "        self._plot_clustering(X, it = it, cls_labels=self.cls_labels)\n",
    "        clusters = [[] for i in range(len(self.centroids))]\n",
    "        it = it + 1\n",
    "\n",
    "        for i in range(len(X)):\n",
    "          for j in range(len(new_centroids)):\n",
    "            df[j][i] = KMeans._euclidean(KMeans(self.n_clusters), X[i], new_centroids[j])\n",
    "        \n",
    "        for i in range(df.T.shape[1]):\n",
    "          filter = df.T[i]==min(df.T[i])\n",
    "          for j in range(len(filter)):\n",
    "            if(filter[j] == True):\n",
    "              self.cls_labels[i] = j\n",
    "              clusters[j].append(X[i])\n",
    "        \n",
    "        for i in range(len(self.centroids)):\n",
    "          self.centroids[i] = np.mean(clusters[i], axis = 0)\n",
    "        \n",
    "        self._plot_clustering(X, it = it, cls_labels=self.cls_labels)\n",
    "        it = it + 1\n",
    "    \n",
    "          \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MinMaxScaler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mloadtxt(\u001b[39m'\u001b[39m\u001b[39mwine-clustering.csv\u001b[39m\u001b[39m'\u001b[39m, skiprows\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, delimiter\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m mm \u001b[39m=\u001b[39m MinMaxScaler()\n\u001b[1;32m      3\u001b[0m mm_scale \u001b[39m=\u001b[39m mm\u001b[39m.\u001b[39mfit(data)\n\u001b[1;32m      4\u001b[0m data_mm \u001b[39m=\u001b[39m mm_scale\u001b[39m.\u001b[39mtransform(data)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MinMaxScaler' is not defined"
     ]
    }
   ],
   "source": [
    "data = np.loadtxt('wine-clustering.csv', skiprows=1, delimiter=',')\n",
    "mm = MinMaxScaler()\n",
    "mm_scale = mm.fit(data)\n",
    "data_mm = mm_scale.transform(data)\n",
    "\n",
    "reduced_data, explained_variance, cumulative_variance = PCA._fit(PCA(data=data))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * First try it out without the scaling. The PCA cumulative variance matrix, which can be seen that even with one variable the whole data can be explained 99%. But in order to obtain the coordinate distances first two variables are obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cumulative_variance)\n",
    "print(cumulative_variance)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The data before the PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data[:,0],data[:,1], c=np.ones(len(data)))\n",
    "plt.title(\"Scatter plot of the data before PCA\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The data after the PCA. We can observe that the data is not distinguished from each other. Clusters cannot be observed easily by reading the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(reduced_data[:,0],reduced_data[:,1], c=np.ones(len(reduced_data)))\n",
    "plt.title(\"Scatter plot of the data after PCA\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In this part, each step of the distance algorithm can be observed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KMeans.fit(KMeans(3), reduced_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This step is for comparing the implementation with the libraries that are in used. As can be seen, there are no difference between the plots, the accuracy nor the PCA results. Also, the 3 clustered non scaled data has 0.57 silhoutte coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans as km\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "kmeans = km(n_clusters=3, init='k-means++', max_iter=300, n_init=10, random_state=1)\n",
    "kmeans.fit(reduced_data)\n",
    "\n",
    "y = kmeans.fit_predict(reduced_data)\n",
    "plt.figure(figsize=(10, 6), dpi=80)\n",
    "plt.scatter(reduced_data[:, 0], reduced_data[:, 1], c=y, s=50, cmap='viridis')\n",
    "\n",
    "centers = kmeans.cluster_centers_\n",
    "labels = kmeans.labels_\n",
    "score = metrics.silhouette_score(reduced_data, labels, metric=\"euclidean\")\n",
    "score1 = metrics.calinski_harabasz_score(reduced_data, labels)\n",
    "score2 = metrics.davies_bouldin_score(reduced_data, labels)\n",
    "print(\"S C: \", score)\n",
    "print(\"CHI: \", score1)\n",
    "print(\"DBI: \", score2)\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='red', marker='x', s=200, alpha=0.5)\n",
    "plt.show()\n",
    "\n",
    "from sklearn.decomposition import PCA as p\n",
    "pca = p()\n",
    "components = p.fit_transform(pca,data)\n",
    "\n",
    "print(reduced_data[0][0])\n",
    "print(components[0][0])\n",
    "\n",
    "print(reduced_data[1][0])\n",
    "print(components[1][0])\n",
    "\n",
    "print(reduced_data[1][1])\n",
    "print(components[1][1])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's try for the scaled version data\n",
    "* As we can observe, 2 variables here only represents the almost 60% of our data. Let's go on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_data, explained_variance, cumulative_variance = PCA._fit(PCA(data=data_mm))\n",
    "plt.plot(cumulative_variance)\n",
    "print(cumulative_variance)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's make the scatter plot comparison between the Scatter Plots of before and after PCA Analaysis\n",
    "* We can clearly observe the clusters right now in the after PCA scatter plot. Let's go on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data[:,0],data[:,1], c=np.ones(len(data)))\n",
    "plt.title(\"Scatter plot of the data before PCA\")\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(reduced_data[:,0],reduced_data[:,1], c=np.ones(len(reduced_data)))\n",
    "plt.title(\"Scatter plot of the data after PCA\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now we can see the difference between clusters better than the first example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KMeans.fit(KMeans(3), reduced_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's check for the accuracy of our implementation again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans as km\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "kmeans = km(n_clusters=3, init='k-means++', max_iter=300, n_init=10, random_state=1)\n",
    "kmeans.fit(reduced_data)\n",
    "\n",
    "y = kmeans.fit_predict(reduced_data)\n",
    "plt.figure(figsize=(10, 6), dpi=80)\n",
    "plt.scatter(reduced_data[:, 0], reduced_data[:, 1], c=y, s=50, cmap='viridis')\n",
    "\n",
    "centers = kmeans.cluster_centers_\n",
    "labels = kmeans.labels_\n",
    "score = metrics.silhouette_score(reduced_data, labels, metric=\"euclidean\")\n",
    "score1 = metrics.calinski_harabasz_score(reduced_data, labels)\n",
    "score2 = metrics.davies_bouldin_score(reduced_data, labels)\n",
    "print(\"S C: \", score)\n",
    "print(\"CHI: \", score1)\n",
    "print(\"DBI: \", score2)\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='red', marker='x', s=200, alpha=0.5)\n",
    "plt.show()\n",
    "\n",
    "from sklearn.decomposition import PCA as p\n",
    "pca = p()\n",
    "components = p.fit_transform(pca,data)\n",
    "\n",
    "print(reduced_data[0][0])\n",
    "print(components[0][0])\n",
    "\n",
    "print(reduced_data[1][0])\n",
    "print(components[1][0])\n",
    "\n",
    "print(reduced_data[1][1])\n",
    "print(components[1][1])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interestingly, the silhoutte coefficients, chi and dbi coefficients resulted better in the without scale part. The reason behind it might be about the explainibility of our variances. Also, despite being an obvious cut between the clusters, the k-means algorithm is not resistent to the outliers. Hence, the outliers of each cluster causes the merging problem of the clusters.\n",
    "\n",
    "Thank you for reading. Waiting for the feedbacks. Good luck!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
